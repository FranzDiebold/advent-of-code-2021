{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ca63d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def prepare_files(file_name):\n",
    "    base_name = file_name.split(\".\")[0]\n",
    "    with open(file_name, \"r\") as file:\n",
    "        chosen_numbers = [number for number in next(file).strip().split(\",\")]\n",
    "        with open(f\"{base_name}/chosen_numbers.txt\", \"w\") as chosen_numbers_file:\n",
    "            chosen_numbers_file.write(\",\".join(chosen_numbers))\n",
    "\n",
    "        board_idx = 0\n",
    "        current_file_content = []\n",
    "        for line in chain(file, [\"\"]):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if current_file_content:\n",
    "                    with open(f\"{base_name}/{base_name}_{board_idx}.txt\", \"w\") as current_file:\n",
    "                        current_file.write(\"\\n\".join(current_file_content))\n",
    "                    current_file_content = []\n",
    "                    board_idx += 1\n",
    "            else:\n",
    "                current_file_content.append(line)\n",
    "\n",
    "prepare_files(\"test-input.txt\")\n",
    "prepare_files(\"input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9be54b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e83fcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/13 09:45:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"advent-of-code-2021\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1e95b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_boards(path):\n",
    "    return spark.read.text(path) \\\n",
    "        .withColumn(\"board_idx\", F.regexp_extract(F.input_file_name(), \"^.*input_(\\\\d+)\\.txt$\", 1).cast(\"int\"))\n",
    "\n",
    "def load_chosen_numbers(path):\n",
    "    with open(f\"{path}/chosen_numbers.txt\", \"r\") as file:\n",
    "        return [int(number) for number in next(file).strip().split(\",\")]\n",
    "\n",
    "def load_data(path):\n",
    "    return load_boards(path), load_chosen_numbers(path)\n",
    "\n",
    "df_test, chosen_numbers_test = load_data(\"test-input/\")\n",
    "df, chosen_numbers = load_data(\"input/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf863700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import add, or_\n",
    "\n",
    "board_size = 5\n",
    "number_mark = -1\n",
    "\n",
    "def prepare_boards(df):\n",
    "    columns = [\n",
    "        \"board_idx\",\n",
    "        (F.row_number().over(Window.partitionBy(\"board_idx\").orderBy(F.lit(1))) - 1).alias(\"row_idx\")\n",
    "    ]\n",
    "    for idx in range(board_size):\n",
    "        columns.append(\n",
    "            F.split(F.trim(\"value\"), \"\\s+\") \\\n",
    "                .getItem(idx) \\\n",
    "                .cast(\"int\") \\\n",
    "                .alias(f\"col_{idx}\")\n",
    "        )\n",
    "    return df.select(*columns)\n",
    "\n",
    "col_columns = [f\"col_{idx}\" for idx in range(board_size)]\n",
    "def mark_numbers(df, chosen_number):\n",
    "    return df.replace(chosen_number, number_mark, subset=col_columns)\n",
    "\n",
    "def check_board_win(df):\n",
    "    row_wins = df \\\n",
    "        .select([\n",
    "            \"board_idx\",\n",
    "            reduce(add, [F.col(column) for column in col_columns]).alias(\"row_sum\")\n",
    "        ]) \\\n",
    "        .filter(F.col(\"row_sum\") == -board_size) \\\n",
    "        .select(\"board_idx\")\n",
    "\n",
    "    agg_columns = [F.sum(f\"col_{idx}\").alias(f\"col_sum_{idx}\") for idx in range(board_size)]\n",
    "    filter_columns = [F.col(f\"col_sum_{idx}\") == -board_size for idx in range(board_size)]\n",
    "    col_wins = df \\\n",
    "        .groupBy(\"board_idx\") \\\n",
    "        .agg(*agg_columns) \\\n",
    "        .filter(reduce(or_, filter_columns)) \\\n",
    "        .select(\"board_idx\")\n",
    "    return row_wins.union(col_wins) \\\n",
    "        .dropDuplicates()\n",
    "\n",
    "def sum_unmarked_numbers(df, board_idx):\n",
    "    return df \\\n",
    "        .filter(F.col(\"board_idx\") == board_idx) \\\n",
    "        .replace(number_mark, 0, subset=col_columns) \\\n",
    "        .select(reduce(add, [F.col(column) for column in col_columns]).alias(\"row_sum\")) \\\n",
    "        .select(F.sum(\"row_sum\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a87c18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/13 09:45:54 WARN BlockManager: Block rdd_12_3 already exists on this machine; not re-adding it\n",
      "21/12/13 09:45:54 WARN BlockManager: Block rdd_12_2 already exists on this machine; not re-adding it\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: 38594\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "def part_1(df, chosen_numbers):\n",
    "    df = prepare_boards(df).repartition(4, \"board_idx\")\n",
    "    winning_board_idx = None\n",
    "    for chosen_number in chosen_numbers:\n",
    "        df = mark_numbers(df, chosen_number).cache()\n",
    "        board_wins = check_board_win(df)\n",
    "        if board_wins.count() > 0:\n",
    "            winning_board_idx = board_wins.take(1)[0][0]\n",
    "            break\n",
    "\n",
    "    if winning_board_idx is None:\n",
    "        raise ValueError(\"No winning board found!\")\n",
    "\n",
    "    numbers_sum = sum_unmarked_numbers(df, winning_board_idx).take(1)[0][0]\n",
    "\n",
    "    return numbers_sum * chosen_number\n",
    "\n",
    "assert part_1(df_test, chosen_numbers_test) == 4512\n",
    "print(f\"Solution: {part_1(df, chosen_numbers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed781e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/13 09:46:42 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:42 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:43 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:44 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:44 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:45 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:46 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:47 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:47 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:48 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:49 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:50 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:55 WARN CacheManager: Asked to cache already cached data.        \n",
      "21/12/13 09:46:56 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:56 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:57 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:58 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:58 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:46:59 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:47:00 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:47:01 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:47:01 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:47:02 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:47:03 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:47:03 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:47:04 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:47:05 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:47:06 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:47:07 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:47:07 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:47:08 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:47:09 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:47:10 WARN CacheManager: Asked to cache already cached data.\n",
      "21/12/13 09:47:11 WARN BlockManager: Block rdd_1372_0 already exists on this machine; not re-adding it\n",
      "21/12/13 09:49:19 WARN DAGScheduler: Broadcasting large task binary with size 1005.2 KiB\n",
      "21/12/13 09:49:24 WARN DAGScheduler: Broadcasting large task binary with size 1017.3 KiB\n",
      "21/12/13 09:49:29 WARN DAGScheduler: Broadcasting large task binary with size 1029.4 KiB\n",
      "21/12/13 09:49:34 WARN DAGScheduler: Broadcasting large task binary with size 1041.5 KiB\n",
      "21/12/13 09:49:35 WARN DAGScheduler: Broadcasting large task binary with size 1002.6 KiB\n",
      "21/12/13 09:49:39 WARN DAGScheduler: Broadcasting large task binary with size 1053.6 KiB\n",
      "21/12/13 09:49:40 WARN DAGScheduler: Broadcasting large task binary with size 1014.7 KiB\n",
      "21/12/13 09:49:44 WARN DAGScheduler: Broadcasting large task binary with size 1065.7 KiB\n",
      "21/12/13 09:49:46 WARN DAGScheduler: Broadcasting large task binary with size 1026.8 KiB\n",
      "21/12/13 09:49:49 WARN DAGScheduler: Broadcasting large task binary with size 1077.8 KiB\n",
      "21/12/13 09:49:51 WARN DAGScheduler: Broadcasting large task binary with size 1038.9 KiB\n",
      "21/12/13 09:49:54 WARN DAGScheduler: Broadcasting large task binary with size 1089.8 KiB\n",
      "21/12/13 09:49:56 WARN DAGScheduler: Broadcasting large task binary with size 1051.0 KiB\n",
      "21/12/13 09:50:00 WARN DAGScheduler: Broadcasting large task binary with size 1101.9 KiB\n",
      "21/12/13 09:50:02 WARN DAGScheduler: Broadcasting large task binary with size 1063.1 KiB\n",
      "21/12/13 09:50:05 WARN DAGScheduler: Broadcasting large task binary with size 1114.0 KiB\n",
      "21/12/13 09:50:07 WARN DAGScheduler: Broadcasting large task binary with size 1075.2 KiB\n",
      "21/12/13 09:50:11 WARN DAGScheduler: Broadcasting large task binary with size 1126.1 KiB\n",
      "21/12/13 09:50:13 WARN DAGScheduler: Broadcasting large task binary with size 1087.3 KiB\n",
      "21/12/13 09:50:16 WARN DAGScheduler: Broadcasting large task binary with size 1068.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: 21184\n"
     ]
    }
   ],
   "source": [
    "# Part 2\n",
    "def part_2(df, chosen_numbers):\n",
    "    df = prepare_boards(df).repartition(4, \"board_idx\")\n",
    "    total_boards_count = df.agg(F.countDistinct(\"board_idx\")).collect()[0][0]\n",
    "    previous_winning_boards = set()\n",
    "    last_winning_board_idx = None\n",
    "    last_winning_chosen_number = None\n",
    "    for chosen_number in chosen_numbers:\n",
    "        df = mark_numbers(df, chosen_number).cache()\n",
    "        board_wins = check_board_win(df)\n",
    "        current_winning_boards = set(board[0] for board in board_wins.collect())\n",
    "\n",
    "        if len(current_winning_boards) == total_boards_count:\n",
    "            last_winning_board_idx = (current_winning_boards - previous_winning_boards).pop()\n",
    "            last_winning_chosen_number = chosen_number\n",
    "            break\n",
    "        previous_winning_boards = current_winning_boards\n",
    "\n",
    "    if last_winning_board_idx is None:\n",
    "        raise ValueError(\"No winning board found!\")\n",
    "\n",
    "    numbers_sum = sum_unmarked_numbers(df, last_winning_board_idx).take(1)[0][0]\n",
    "\n",
    "    return numbers_sum * last_winning_chosen_number\n",
    "\n",
    "assert part_2(df_test, chosen_numbers_test) == 1924\n",
    "print(f\"Solution: {part_2(df, chosen_numbers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf7adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
